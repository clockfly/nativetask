Index: src/core/org/apache/hadoop/io/compress/snappy/LoadSnappy.java
===================================================================
--- src/core/org/apache/hadoop/io/compress/snappy/LoadSnappy.java	
+++ src/core/org/apache/hadoop/io/compress/snappy/LoadSnappy.java	
@@ -36,7 +36,7 @@
       LOG.warn("Snappy native library is available");
       AVAILABLE = true;
     } catch (UnsatisfiedLinkError ex) {
-      //NOP
+      LOG.error("Failed to load snappy with error: " + ex);
     }
     boolean hadoopNativeAvailable = NativeCodeLoader.isNativeCodeLoaded();
     LOADED = AVAILABLE && hadoopNativeAvailable;
Index: src/core/org/apache/hadoop/io/Text.java
===================================================================
--- src/core/org/apache/hadoop/io/Text.java	
+++ src/core/org/apache/hadoop/io/Text.java	
@@ -206,6 +206,10 @@
     this.length = len;
   }
 
+  public void setLength(int len) {
+    this.length = len;
+  }
+  
   /**
    * Append a range of bytes to the end of the given text
    * @param utf8 the data to copy from
@@ -235,7 +239,7 @@
    * @param len the number of bytes we need
    * @param keepData should the old data be kept
    */
-  private void setCapacity(int len, boolean keepData) {
+  public void setCapacity(int len, boolean keepData) {
     if (bytes == null || bytes.length < len) {
       byte[] newBytes = new byte[len];
       if (bytes != null && keepData) {
Index: src/core/org/apache/hadoop/util/NativeCodeLoader.java
===================================================================
--- src/core/org/apache/hadoop/util/NativeCodeLoader.java	
+++ src/core/org/apache/hadoop/util/NativeCodeLoader.java	
@@ -37,15 +37,15 @@
   
   static {
     // Try to load native hadoop library and set fallback flag appropriately
-    LOG.debug("Trying to load the custom-built native-hadoop library...");
+    LOG.info("Trying to load the custom-built native-hadoop library...");
     try {
       System.loadLibrary("hadoop");
       LOG.info("Loaded the native-hadoop library");
       nativeCodeLoaded = true;
     } catch (Throwable t) {
       // Ignore failure to load
-      LOG.debug("Failed to load native-hadoop with error: " + t);
-      LOG.debug("java.library.path=" + System.getProperty("java.library.path"));
+      LOG.error("Failed to load native-hadoop with error: " + t);
+      LOG.error("java.library.path=" + System.getProperty("java.library.path"));
     }
     
     if (!nativeCodeLoaded) {
Index: src/core/org/apache/hadoop/util/ResourceCalculatorPlugin.java
===================================================================
--- src/core/org/apache/hadoop/util/ResourceCalculatorPlugin.java	
+++ src/core/org/apache/hadoop/util/ResourceCalculatorPlugin.java	
@@ -141,6 +141,9 @@
     if (clazz != null) {
       return ReflectionUtils.newInstance(clazz, conf);
     }
+    if (true) {
+      return null;
+    }
 
     // No class given, try a os specific class
     try {
Index: src/mapred/org/apache/hadoop/mapred/Task.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/Task.java
+++ src/mapred/org/apache/hadoop/mapred/Task.java
@@ -538,7 +538,7 @@
     }
   }
   
-  protected class TaskReporter 
+  public class TaskReporter 
       extends org.apache.hadoop.mapreduce.StatusReporter
       implements Runnable, Reporter {
     private TaskUmbilicalProtocol umbilical;
@@ -1351,7 +1351,7 @@
     }
   }
 
-  protected static abstract class CombinerRunner<K,V> {
+  public static abstract class CombinerRunner<K,V> {
     protected final Counters.Counter inputCounter;
     protected final JobConf job;
     protected final TaskReporter reporter;
@@ -1369,13 +1369,13 @@
      * @param iterator the key/value pairs to use as input
      * @param collector the output collector
      */
-    abstract void combine(RawKeyValueIterator iterator, 
+    public abstract void combine(RawKeyValueIterator iterator, 
                           OutputCollector<K,V> collector
                          ) throws IOException, InterruptedException, 
                                   ClassNotFoundException;
 
     @SuppressWarnings("unchecked")
-    static <K,V> 
+    public static <K,V> 
     CombinerRunner<K,V> create(JobConf job,
                                TaskAttemptID taskId,
                                Counters.Counter inputCounter,
@@ -1422,7 +1422,7 @@
     }
 
     @SuppressWarnings("unchecked")
-    protected void combine(RawKeyValueIterator kvIter,
+    public void combine(RawKeyValueIterator kvIter,
                            OutputCollector<K,V> combineCollector
                            ) throws IOException {
       Reducer<K,V,K,V> combiner = 
@@ -1489,7 +1489,7 @@
 
     @SuppressWarnings("unchecked")
     @Override
-    void combine(RawKeyValueIterator iterator, 
+    public void combine(RawKeyValueIterator iterator, 
                  OutputCollector<K,V> collector
                  ) throws IOException, InterruptedException,
                           ClassNotFoundException {
Index: src/mapred/org/apache/hadoop/mapred/ReduceTask.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/ReduceTask.java
+++ src/mapred/org/apache/hadoop/mapred/ReduceTask.java
@@ -75,6 +75,7 @@
 import org.apache.hadoop.mapred.IFile.*;
 import org.apache.hadoop.mapred.Merger.Segment;
 import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
+import org.apache.hadoop.mapred.TaskDelegation.ReduceTaskDelegator;
 import org.apache.hadoop.mapred.TaskTracker.TaskInProgress;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.metrics2.MetricsBuilder;
@@ -414,7 +415,14 @@
     Class valueClass = job.getMapOutputValueClass();
     RawComparator comparator = job.getOutputValueGroupingComparator();
 
-    if (useNewApi) {
+    ReduceTaskDelegator reduceDelegator = TaskDelegation.getReduceTaskDelegator(umbilical, reporter, job);
+    
+    if (null != reduceDelegator) {
+      reduceDelegator.run(this.getTaskID(), 
+          rIter, comparator, 
+          keyClass, valueClass);
+      
+    } else if (useNewApi) {
       runNewReducer(job, umbilical, reporter, rIter, comparator, 
                     keyClass, valueClass);
     } else {
Index: src/mapred/org/apache/hadoop/mapred/MapTask.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapTask.java
+++ src/mapred/org/apache/hadoop/mapred/MapTask.java
@@ -61,6 +61,7 @@
 import org.apache.hadoop.mapred.IFile.Writer;
 import org.apache.hadoop.mapred.Merger.Segment;
 import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
+import org.apache.hadoop.mapred.TaskDelegation.MapTaskDelegator;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.split.JobSplit.TaskSplitIndex;
 import org.apache.hadoop.util.IndexedSortable;
@@ -71,14 +72,14 @@
 import org.apache.hadoop.util.StringUtils;
 
 /** A Map task. */
-class MapTask extends Task {
+public class MapTask extends Task {
   /**
    * The size of each record in the index file for the map-outputs.
    */
   public static final int MAP_OUTPUT_INDEX_RECORD_LENGTH = 24;
 
   private TaskSplitIndex splitMetaInfo = new TaskSplitIndex();
-  private final static int APPROX_HEADER_LENGTH = 150;
+  public final static int APPROX_HEADER_LENGTH = 150;
 
   private static final Log LOG = LogFactory.getLog(MapTask.class.getName());
 
@@ -359,8 +360,14 @@
       runTaskCleanupTask(umbilical, reporter);
       return;
     }
-
-    if (useNewApi) {
+    
+    MapTaskDelegator mapTaskDelegator = TaskDelegation.getMapTaskDelegator(umbilical, reporter, job);
+    
+    if (null != mapTaskDelegator) {
+      mapTaskDelegator.run(this.getTaskID(),  getSplitDetails(
+        new Path(splitMetaInfo.getSplitLocation()),
+        splitMetaInfo.getStartOffset()));
+    } else if (useNewApi) {
       runNewMapper(job, splitMetaInfo, umbilical, reporter);
     } else {
       runOldMapper(job, splitMetaInfo, umbilical, reporter);
@@ -419,7 +419,11 @@ class MapTask extends Task {
     LOG.info("numReduceTasks: " + numReduceTasks);
     MapOutputCollector collector = null;
     if (numReduceTasks > 0) {
-      collector = new MapOutputBuffer(umbilical, job, reporter);
+      collector = TaskDelegation.getOutputCollectorDelegator(umbilical, reporter, job, this);
+      if (collector == null) {
+        LOG.warn("Nativetask falling back to Java MapOutputCollector");
+        collector = new MapOutputBuffer(umbilical, job, reporter);
+      }
     } else { 
       collector = new DirectMapOutputCollector(umbilical, job, reporter);
     }
@@ -670,7 +680,14 @@
                        TaskUmbilicalProtocol umbilical,
                        TaskReporter reporter
                        ) throws IOException, ClassNotFoundException {
-      collector = new MapOutputBuffer<K,V>(umbilical, job, reporter);
+      MapOutputCollector<K,V> delegateCollector = TaskDelegation.getOutputCollectorDelegator(umbilical, reporter, job, MapTask.this);
+  	   if (null == delegateCollector) {
+        LOG.warn("Nativetask falling back to Java MapOutputCollector");
+        collector = new MapOutputBuffer<K,V>(umbilical, job, reporter);
+  	   } else {
+  	     collector = delegateCollector;
+  	   }
+  	  
       partitions = jobContext.getNumReduceTasks();
       if (partitions > 0) {
         partitioner = (org.apache.hadoop.mapreduce.Partitioner<K,V>)
Index: src/core/org/apache/hadoop/io/BooleanWritable.java
===================================================================
--- src/core/org/apache/hadoop/io/BooleanWritable.java
+++ src/core/org/apache/hadoop/io/BooleanWritable.java
@@ -100,9 +100,7 @@
 
     public int compare(byte[] b1, int s1, int l1,
                        byte[] b2, int s2, int l2) {
-      boolean a = (readInt(b1, s1) == 1) ? true : false;
-      boolean b = (readInt(b2, s2) == 1) ? true : false;
-      return ((a == b) ? 0 : (a == false) ? -1 : 1);
+      return compareBytes(b1, s1, l1, b2, s2, l2); 
     }
   }
 
Index: src/mapred/org/apache/hadoop/mapred/TaskDelegation.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskDelegation.java
+++ src/mapred/org/apache/hadoop/mapred/TaskDelegation.java
@@ -0,0 +1,164 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.Task.TaskReporter;
+import org.apache.hadoop.util.ReflectionUtils;
+
+public class TaskDelegation {
+
+  private static final Log LOG = LogFactory.getLog(TaskDelegation.class);
+  
+  public final static String MAP_TASK_DELEGATPR = "mapreduce.map.task.delegator.class";
+  public final static String REDUCE_TASK_DELEGATPR = "mapreduce.reduce.task.delegator.class";
+  public final static String MAP_OUTPUT_COLLECTOR_DELEGATPR = "mapreduce.map.output.collector.delegator.class";  
+
+  /**
+   * inteface for map task delegator
+   *
+   */
+  public static interface MapTaskDelegator {
+    
+    public void init(TaskUmbilicalProtocol umbilical, TaskReporter reporter, Configuration conf) 
+        throws Exception;
+    
+    public void run(TaskAttemptID taskID,  
+        Object split)
+            throws IOException;
+  }
+  
+  /**
+   * inteface for map reduce task delegator
+   *
+   */
+  public static interface ReduceTaskDelegator {
+
+    public void init(TaskUmbilicalProtocol umbilical, TaskReporter reporter, Configuration conf) 
+        throws Exception;
+    
+    public void run(TaskAttemptID taskID, 
+        RawKeyValueIterator rIter, RawComparator comparator, 
+        Class keyClass, Class valueClass)
+        throws IOException;
+  }
+
+  /**
+   * inteface for map output collector delegator
+   *
+   */
+  public static interface MapOutputCollectorDelegator<K, V> extends
+    MapTask.MapOutputCollector<K, V> {
+
+    void init(TaskUmbilicalProtocol umbilical, TaskReporter reporter,
+        Configuration conf, Task task) throws Exception;
+  }
+  
+  public static MapTaskDelegator getMapTaskDelegator(TaskUmbilicalProtocol protocol
+      , TaskReporter reporter, JobConf job) {
+    String delegateMapClazz = job.get(MAP_TASK_DELEGATPR, null);
+    if (null == delegateMapClazz || delegateMapClazz.isEmpty()
+        || delegateMapClazz.equals("dummy")) {
+      LOG.info("MapTaskDelegator is not defined");
+      return null;
+    }
+    
+    MapTaskDelegator delegator = null;
+    try {
+      Class<? extends MapTaskDelegator> delegatorClass = (Class<? extends MapTaskDelegator>) 
+          job.getClass(MAP_TASK_DELEGATPR, null);
+      if (null == delegatorClass) {
+        LOG.info("MapTaskDelegator class cannot be load " + delegateMapClazz);
+        return null;
+      }
+      delegator = ReflectionUtils.newInstance(delegatorClass, job);
+      delegator.init(protocol, reporter, job);
+      LOG.info("MapTaskDelegator " + delegateMapClazz + " is enabled");
+    }
+    catch(Exception e) {
+      LOG.error("MapTaskDelegator " + delegateMapClazz + " is not enabled", e);
+      return null;
+    }
+    return delegator;
+  }
+
+
+  public static ReduceTaskDelegator getReduceTaskDelegator(TaskUmbilicalProtocol protocol
+      , TaskReporter reporter, JobConf job) {
+    String delegateReducerClazz = job.get(REDUCE_TASK_DELEGATPR, null);
+    if (null == delegateReducerClazz || delegateReducerClazz.isEmpty() 
+        || delegateReducerClazz.equals("dummy")) {
+      LOG.info("Reduce task Delegator not defined");
+      return null;
+    }
+    
+    ReduceTaskDelegator delegator = null;
+    try {
+      Class<? extends ReduceTaskDelegator> delegatorClass = (Class<? extends ReduceTaskDelegator>) 
+          job.getClass(REDUCE_TASK_DELEGATPR, null);
+      if (null == delegatorClass) {
+        LOG.info("Reduce task Delegator class cannot be load " + delegateReducerClazz);
+        return null;
+      }
+      delegator = ReflectionUtils.newInstance(delegatorClass, job);
+      delegator.init(protocol, reporter, job);
+      LOG.info("ReduceTaskDelegator " + delegateReducerClazz + " is enabled");
+    }
+    catch(Exception e) {
+      LOG.warn("ReduceTaskDelegator " + delegateReducerClazz + " is not enabled", e);
+      return null;
+    }
+    return delegator;
+  }
+
+  @SuppressWarnings("unchecked")
+  public static <K, V> MapOutputCollectorDelegator<K, V> 
+      getOutputCollectorDelegator(TaskUmbilicalProtocol protocol
+          , TaskReporter reporter, JobConf job, Task task) {
+    
+    String delegatorClazz = job.get(MAP_OUTPUT_COLLECTOR_DELEGATPR, null);
+    if (null == delegatorClazz || delegatorClazz.isEmpty() || delegatorClazz.equals("dummy")) {
+      LOG.info("MapOutputCollectorDelegator not found");
+      return null;
+    }
+
+    MapOutputCollectorDelegator delegator = null;
+    try {
+      Class<? extends MapOutputCollectorDelegator> delegatorClass = (Class<? extends MapOutputCollectorDelegator>) 
+          job.getClass(MAP_OUTPUT_COLLECTOR_DELEGATPR, null);
+      if (null == delegatorClass) {
+        LOG.info("MapOutputCollectorDelegator cannot be inited " + delegatorClazz);
+        return null;
+      }
+      delegator = ReflectionUtils.newInstance(delegatorClass, job);
+      delegator.init(protocol, reporter, job, task);
+      LOG.info("MapOutputCollectorDelegator " + delegatorClazz + " is enabled");
+    }
+    catch(Exception e) {
+      LOG.error("MapOutputCollectorDelegator " + delegatorClazz + " is not enabled", e);
+      return null;
+    }
+    return delegator;
+  }
+}
Index: build.xml
===================================================================
--- build.xml
+++ build.xml
@@ -756,10 +756,22 @@
   <target name="compile-contrib" depends="compile-core,tools-jar,compile-c++-libhdfs">
      <subant target="compile">
         <property name="version" value="${version}"/>
+		<property name="dist.dir" value="${dist.dir}"/>
+		<property name="build.platform" value="${build.platform}"/>
+		
         <fileset file="${contrib.dir}/build.xml"/>
      </subant>  	
   </target>
   
+  <target name="package-contrib">
+     <subant target="package">        
+		<property name="version" value="${version}"/>
+		<property name="dist.dir" value="${dist.dir}"/>
+		<property name="build.platform" value="${build.platform}"/>
+        <fileset file="src/contrib/build.xml"/>
+     </subant>  	
+  </target>
+  
   <target name="compile" depends="compile-core, compile-contrib, compile-ant-tasks, compile-tools" description="Compile core, contrib">
   </target>
 
@@ -1248,6 +1260,8 @@
     <subant target="test">
        <property name="version" value="${version}"/>
        <property name="clover.jar" value="${clover.jar}"/>
+		<property name="dist.dir" value="${dist.dir}"/>
+		<property name="build.platform" value="${build.platform}"/>
        <fileset file="${contrib.dir}/build.xml"/>
     </subant> 
   </target>
@@ -1593,6 +1607,7 @@
       distribution directory so contribs know where to install to.-->
       <property name="version" value="${version}"/>
       <property name="dist.dir" value="${dist.dir}"/>
+	  <property name="build.platform" value="${build.platform}"/>
       <fileset file="${contrib.dir}/build.xml"/>
     </subant>  	
 
@@ -1793,6 +1808,7 @@
       distribution directory so contribs know where to install to.-->
       <property name="version" value="${version}"/>
       <property name="dist.dir" value="${dist.dir}/share/${name}"/>
+	  <property name="build.platform" value="${build.platform}"/>
       <fileset file="${contrib.dir}/build.xml"/>
     </subant>  	
 
